#install.packages("foreign")
#install.packages("caret")
#install.packages("car")
#install.packages("nlme")
#install.packages("rms")
#install.packages("e1071")
#install.packages("BiodiversityR")
#install.packages("moments")
#install.packages("randomForest")
#install.packages("dplyr")
#install.packages("DMwR")
#install.packages("xgboost")

library(foreign)
library(caret)
library(car)
library(nlme)
library(rms)
library(e1071)
library(BiodiversityR)
library(moments)
library(randomForest)
library(dplyr)
library(DMwR)
library(rpart)
require(xgboost)

setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

data <- read.csv("data2/raw_hadoop.csv", header=T,sep=",")
data <- subset(data, select = -c(author_date,author_name,author_date_unix_timestamp,author_email,commit_message,fix,classification,linked,fixes,fileschanged,repository_id,glm_probability,rf_probability,issue_id,issue_date,issue_type))

data <- data[!(is.na(data$contains_bug)),]
#data <- data[,colSums(is.na(data))<nrow(data)]


#CHANGE THE TYPES OF COLUMNS TO ALLOW SMOTE-ING (SMOTE HATES CHR)
data$contains_bug = as.factor(data$contains_bug)
data$commit_hash = as.factor(data$commit_hash)
data$perf_type = as.factor(data$perf_type)

#SMOTE IN ORDER
balanced.data <- SMOTE(contains_bug~., data, perc.over=100)
data <- SMOTE(perf_type ~., balanced.data, perc.over=100)

# PUT THINGS BACK TO THEIR ORIGINAL TYPES
data$contains_bug = as.numeric(levels(data$contains_bug))[data$contains_bug]
data$perf_type = as.numeric(levels(data$perf_type))[data$perf_type]
data$commit_hash = as.character(levels(data$commit_hash))[data$commit_hash]

#THIS IS JUST TO CHECK IF THINGS ARE OK, CAN BE DELETED
#str(data)

#START THE THING
k=10 #Folds

id <- sample(1:k,nrow(data),replace=TRUE)
list <- 1:k
trainingset <- data.frame()
testset <- data.frame()

fit=c();

precision_rf=c();
recall_rf=c();

for (i in 1:k)
{
  
  trainingset <- subset(data, id %in% list[-i])
  trainingset <- subset(trainingset, select = -c(commit_hash))
  testset <- subset(data, id %in% c(i))
  file2 <- paste("hadoop_test_", i, sep="")
  file2 <- paste(file2, ".csv", sep="")
  write.csv(testset, file2, row.names = FALSE)
  
  testset <- subset(testset, select = -c(commit_hash))
  
  
  drop=c("contains_bug")
  also_drop=c("perf_type")

  independant=trainingset[,!(names(trainingset) %in% drop)]
  independant=independant[,!(names(independant) %in% also_drop)]

  ##########correlation 
  correlations <- cor(independant, method="spearman") 
  highCorr <- findCorrelation(correlations, cutoff = .75)
  
  low_cor_names=names(independant[, -highCorr])
  low_cor_data= independant[(names(independant) %in% low_cor_names)]
  dataforredun=low_cor_data
  
  #########start redun
  redun_obj = redun (~. ,data = dataforredun ,nk =0)
  after_redun= dataforredun[,!(names(dataforredun) %in% redun_obj $Out)]
  
  ############model
  file <- paste("hadoop_prediction_", i, sep="")
  file <- paste(file, ".csv", sep="")
  
  #form=as.formula(paste("contains_bug>0~",paste(names(after_redun),collapse="+")))
  #rf.fit=svm(formula=form, data=log10(trainingset+1),type="C-classification")
  bstDense <- xgboost(data = as.matrix(log10(trainingset+1)), label = train$label, max_depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
  #rf_predictions <- predict(bstDense, log10(testset+1))
  
  #TP_rf = sum((rf_predictions=="TRUE") & (testset$contains_bug>0))
  #FP_rf = sum((rf_predictions=="TRUE") & (testset$contains_bug==0))
  
  #precision_rf[i] = TP_rf / sum((rf_predictions=="TRUE"))
  #recall_rf[i] = TP_rf / sum(testset$contains_bug>0)
  
  #x <- data.frame(rf_predictions)
  
  #write.table(x, file, sep = ",", append = TRUE, quote = FALSE, col.names=c("Prediction"), row.names = FALSE)
  
 
  
}



