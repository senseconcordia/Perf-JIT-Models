#install.packages("foreign")
#install.packages("caret")
#install.packages("car")
#install.packages("nlme")
#install.packages("rms")
#install.packages("e1071")
#install.packages("BiodiversityR")
#install.packages("moments")
#install.packages("randomForest")
#install.packages("dplyr")
#install.packages("DMwR")

#library(foreign)
#library(caret)
#library(car)
#library(nlme)
#library(rms)
#library(e1071)
#library(BiodiversityR)
#library(moments)
#library(randomForest)
#library(dplyr)
#library(DMwR)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

data <- read.csv("data2/corrected_hadoop.csv", header=T,sep=",")
data <- subset(data, select = -c(author_date,author_name,author_date_unix_timestamp,author_email,commit_message,fix,classification,linked,fixes,fileschanged,repository_id,glm_probability,rf_probability,issue_id,issue_date,issue_type))

data <- data[!(is.na(data$contains_bug)),]
data <- na.omit(data)

#CHANGE THE TYPES OF COLUMNS TO ALLOW SMOTE-ING (SMOTE HATES CHR)
data$contains_bug = as.factor(data$contains_bug)
data$commit_hash = as.factor(data$commit_hash)
#data$perf_type = as.factor(data$perf_type)

#SMOTE IN ORDER
#data <- SMOTE(contains_bug~., data, perc.over=100)
data <- downSample(data, contains_bug)


# PUT THINGS BACK TO THEIR ORIGINAL TYPES
data$contains_bug = as.numeric(levels(data$contains_bug))[data$contains_bug]
#data$perf_type = as.numeric(levels(data$perf_type))[data$perf_type]
data$commit_hash = as.character(levels(data$commit_hash))[data$commit_hash]

#THIS IS JUST TO CHECK IF THINGS ARE OK, CAN BE DELETED
#str(data)

#START THE THING
k=10 #Folds

id <- sample(1:k,nrow(data),replace=TRUE)
list <- 1:k
trainingset <- data.frame()
testset <- data.frame()

fit=c();

precision_rf=c();
recall_rf=c();

for (i in 1:k)
{
  
  trainingset <- subset(data, id %in% list[-i])
  trainingset <- subset(trainingset, select = -c(commit_hash))
  testset <- subset(data, id %in% c(i))
  file2 <- paste("hadoop_test_", i, sep="")
  file2 <- paste(file2, ".csv", sep="")
  write.csv(testset, file2, row.names = FALSE)
  
  testset <- subset(testset, select = -c(commit_hash))
  
  
  drop=c("contains_bug")
  also_drop=c("perf_type")
  
  independant=trainingset[,!(names(trainingset) %in% drop)]
  independant=independant[,!(names(independant) %in% also_drop)]
  
  ##########correlation 
  correlations <- cor(independant, method="spearman") 
  highCorr <- findCorrelation(correlations, cutoff = .75)
  
  low_cor_names=names(independant[, -highCorr])
  low_cor_data= independant[(names(independant) %in% low_cor_names)]
  dataforredun=low_cor_data
  
  #########start redun
  redun_obj = redun (~. ,data = dataforredun ,nk =0)
  after_redun= dataforredun[,!(names(dataforredun) %in% redun_obj $Out)]
  
  
  
  ############model
  file <- paste("hadoop_prediction_", i, sep="")
  file <- paste(file, ".csv", sep="")
  form=as.formula(paste("contains_bug>0~",paste(names(after_redun),collapse="+")))
  model=glm(formula=form, data=log10(trainingset+1), family = binomial(link = "logit"))
  predictions <- predict(model, log10(testset+1) ,type="response")
  x <- data.frame(predictions)
  write.table(x, file, sep = ",", append = TRUE, quote = FALSE, col.names=c("Prediction"), row.names = FALSE)
}



